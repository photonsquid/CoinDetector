{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recoinize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://github.com/nicknochnack/FaceRecognition](https://github.com/nicknochnack/FaceRecognition/blob/main/Facial%20Verification%20with%20a%20Siamese%20Network%20-%20Final.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull code from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.helpers.cmd import git_update, print_gpu_name\n",
    "from src.helpers.install import install_requirements\n",
    "# git_update(\"live-edit\", force=False, show_output=False)\n",
    "# install_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git fetch\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_name()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependencies\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import tensorflow dependencies - Functional API\n",
    "if not testing:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from keras.layers import Dense, Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "    from keras.models import Sequential, load_model, Model\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    from src.models import make_embedding, make_siamese_model\n",
    "    from src.helpers.setup import set_gpus_growth\n",
    "    from src.dataset import create_pairs\n",
    "    from src.preprocess import preprocess\n",
    "\n",
    "    set_gpus_growth()\n",
    "else:\n",
    "    print(\"Testing mode, skipping tensorflow imports\")\n",
    "    from src.helpers.load_data import load_data\n",
    "    from src.dataset import create_pairs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration photonsquid--coins-euro-b75261052d3e19e0\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/photonsquid___parquet/photonsquid--coins-euro-b75261052d3e19e0/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8877a4ed8f84324bab8276a16c6c1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset from HuggingFace\n",
    "if not testing:\n",
    "    dataset = load_dataset('photonsquid/coins-euro')\n",
    "    # convert the dataset to tensorflow dataset\n",
    "    dataset = dataset.with_format('tf')\n",
    "else:\n",
    "    # load data from local files\n",
    "    dataset = load_data(\"data/tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pairs and labels for the training dataset\n",
    "train_dataset = create_p airs(train_dataset)\n",
    "\n",
    "# convert it to a tensorflow dataset\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first pair in subplot with plt\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(train_dataset[0][0][\"bytes\"])\n",
    "# plt.axis('off')\n",
    "# plt.title(\"anchor\")\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(train_dataset[0][1][\"bytes\"])\n",
    "# plt.axis('off')\n",
    "# plt.title(\"positive\" if sum(train_dataset[0][2]) == 5 else \"negative\")\n",
    "# plt.rcParams['figure.facecolor'] = '#282A36'\n",
    "# plt.rcParams['text.color'] = '#ecf0f1'\n",
    "# plt.show()\n",
    "\n",
    "# decode the next image\n",
    "# convert dataset into a numpy iterator\n",
    "train_dataset = train_dataset.as_numpy_iterator()\n",
    "# decode the next image\n",
    "image = next(train_dataset)\n",
    "# convert the bytes to a numpy array\n",
    "image = np.frombuffer(image[0][\"bytes\"], dtype=np.uint8)\n",
    "# plot it with matplotlib\n",
    "# convert the bytes to a numpy array\n",
    "image = image.reshape((256, 256, 3))\n",
    "# plot it\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot it with matplotlib\n",
    "# convert the bytes to a numpy array\n",
    "\n",
    "# image = image.reshape((256, 256, 3))\n",
    "# plot it\n",
    "# plt.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset for training\n",
    "train_dataset = train_dataset.shuffle(1000)\n",
    "train_dataset = train_dataset.batch(32)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# prepare the dataset for testing\n",
    "test_dataset = test_dataset.shuffle(1000)\n",
    "test_dataset = test_dataset.batch(32)\n",
    "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
